{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Part 3\n",
    "# Going Through the Use Cases\n",
    "Here we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see what trying to answer without using RAG looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# an example question about the Amazon reviews\n",
    "query = \"What people hated their food?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You answer questions about Amazon fine food reviews.'},\n",
    "        {'role': 'user', 'content': query},\n",
    "    ],\n",
    "    model='gpt-4-turbo',\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Embeddings\n",
    "The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:\n",
    "\n",
    "| PRODUCT ID  | USER ID        | SCORE | SUMMARY             | TEXT                                           |\n",
    "|-------------|----------------|-------|---------------------|------------------------------------------------|\n",
    "| B001E4KFG0  | A3SGXH7AUHU8GW | 5     | Good Quality Dog Food | I have bought several of the Vitality canned... |\n",
    "| B00813GRG4  | A1D87F6ZCVE5NK | 1     | Not as Advertised   | Product arrived labeled as Jumbo Salted Peanut... |\n",
    "\n",
    "\n",
    "\n",
    "We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to get the file size\n",
    "def get_file_size(file_path):\n",
    "    \"\"\" Returns the size of the file in megabytes. \"\"\"\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)  # Convert from bytes to megabytes\n",
    "    return size_mb\n",
    "\n",
    "# main utility function to get the embeddings\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    # Replace newlines in the text with spaces for consistent formatting\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Request the embedding for the cleaned text and return the embedding\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# utility function to get the embeddings with reduced dimensions\n",
    "def get_embedding_reduced_dims(text, model=\"text-embedding-3-large\"):\n",
    "    # Replace newlines in the text with spaces for consistent formatting\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Request the embedding for the cleaned text and return the embedding\n",
    "    return client.embeddings.create(input=[text], model=model,dimensions=1024).data[0].embedding\n",
    "\n",
    "# utility function to get the embeddings with reduced dimensions\n",
    "def get_embedding_large(text, model=\"text-embedding-3-large\"):\n",
    "    # Replace newlines in the text with spaces for consistent formatting\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Request the embedding for the cleaned text and return the embedding\n",
    "    return client.embeddings.create(input=[text], model=model,dimensions=1024).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the input data file\n",
    "input_data_path = \"./EmbeddingsDemoAssets/fine_food_reviews_1k.csv\"\n",
    "\n",
    "# Read the CSV file using pandas and set the first column as the index\n",
    "df = pd.read_csv(input_data_path, index_col=0)\n",
    "\n",
    "# Select only the relevant columns from the dataframe\n",
    "df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Combine the 'Summary' and 'Text' columns into a new column with a formatted string\n",
    "df[\"combined\"] = (\n",
    "    \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n",
    ")\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Model Embedding\n",
    "Now let's embed using the small model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the `get_embedding` function to each entry in the 'combined' column and store the results\n",
    "df['embedding'] = df['combined'].apply(lambda x: get_embedding(x))\n",
    "\n",
    "# Save the dataframe with embeddings to a CSV file, omitting the index\n",
    "df.to_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_1k.csv', index=False)\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the data file\n",
    "file_path = './EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_1k.csv'\n",
    "size_mb = get_file_size(file_path)\n",
    "print(f\"The size of the file is {size_mb:.2f} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Model Embedding\n",
    "Let's embed using the large model next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the `get_embedding_large` function to each entry in the 'combined' column and store the results\n",
    "df['embedding'] = df['combined'].apply(lambda x: get_embedding_large(x))\n",
    "\n",
    "# Save the dataframe with embeddings to a CSV file, omitting the index\n",
    "df.to_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_large_1k.csv', index=False)\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the data file\n",
    "file_path = './EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_large_1k.csv'\n",
    "size_mb = get_file_size(file_path)\n",
    "print(f\"The size of the file is {size_mb:.2f} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "Here we show how to load the data into a dataframe from a CSV file to make it ready to be used again when needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_load = pd.read_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_large_1k.csv')\n",
    "\n",
    "# Convert the string representation of embeddings in the 'embedding' column to numpy arrays\n",
    "# By converting the embeddings from string format to numpy arrays immediately after loading, \n",
    "# we ensure that the data is in a ready-to-use state for any subsequent analysis or processing steps.\n",
    "df_load['embedding'] = df_load['embedding'].apply(eval).apply(np.array)\n",
    "\n",
    "# Display the first 5 rows of the loaded dataframe\n",
    "df_load.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the Dimensions\n",
    "Using larger embeddings, for example storing them in a vector store for retrieval, generally costs more and consumes more compute, memory and storage than using smaller embeddings.\n",
    "\n",
    "Both of our new embedding models were trained with a technique that allows developers to trade-off performance and cost of using embeddings. Specifically, developers can shorten embeddings (i.e. remove some numbers from the end of the sequence) without the embedding losing its concept-representing properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the input data file\n",
    "input_data_path = \"./EmbeddingsDemoAssets/fine_food_reviews_1k.csv\"\n",
    "\n",
    "# Read the CSV file using pandas and set the first column as the index\n",
    "df_reduced_dims = pd.read_csv(input_data_path, index_col=0)\n",
    "\n",
    "# Select only the relevant columns from the dataframe\n",
    "df_reduced_dims = df_reduced_dims[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_reduced_dims = df_reduced_dims.dropna()\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_reduced_dims = df_reduced_dims.drop_duplicates()\n",
    "\n",
    "# Combine the 'Summary' and 'Text' columns into a new column with a formatted string\n",
    "df_reduced_dims[\"combined\"] = (\n",
    "    \"Title: \" + df_reduced_dims.Summary.str.strip() + \"; Content: \" + df_reduced_dims.Text.str.strip()\n",
    ")\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df_reduced_dims.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the `get_embedding` function to each entry in the 'combined' column and store the results\n",
    "df_reduced_dims['embedding'] = df_reduced_dims['combined'].apply(lambda x: get_embedding_reduced_dims(x))\n",
    "\n",
    "# Save the dataframe with embeddings to a CSV file, omitting the index\n",
    "df_reduced_dims.to_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_reduced_dims_1k.csv', index=False)\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the size of the data file\n",
    "file_path = './EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_reduced_dims_1k.csv'\n",
    "size_mb = get_file_size(file_path)\n",
    "print(f\"The size of the file is {size_mb:.2f} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Using Embeddings-Based Search\n",
    "There are many common cases where the model is not trained on data which contains key facts and information you want to make accessible when generating responses to a user query. One way of solving this is to put additional information into the context window of the model. This is effective in many use cases but leads to higher token costs. The other way is to use RAG to obtain the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_search = pd.read_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_large_1k.csv')\n",
    "\n",
    "# Convert the string representation of embeddings in the 'embedding' column to numpy arrays\n",
    "# By converting the embeddings from string format to numpy arrays immediately after loading, \n",
    "# we ensure that the data is in a ready-to-use state for any subsequent analysis or processing steps.\n",
    "df_search['embedding'] = df_search['embedding'].apply(eval).apply(np.array)\n",
    "\n",
    "# Display the first 5 rows of the loaded dataframe\n",
    "df_search.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function that ranks strings from a pandas DataFrame based on their relatedness to a given query string.\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    "    threshold: float = 0.001  # Minimum score difference to consider for ranking\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"\n",
    "    Retrieve the top 'n' strings related to a query string from a DataFrame, based on a custom relatedness function.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The string to compare other strings against.\n",
    "    df (pd.DataFrame): DataFrame containing the strings and their embeddings.\n",
    "    relatedness_fn (callable): A function that computes the relatedness score between two embeddings. By default, \n",
    "                               it uses the cosine similarity between embeddings.\n",
    "    top_n (int): The number of top related strings to return.\n",
    "    threshold (float): The minimum difference between scores needed to consider one string more related than another.\n",
    "\n",
    "    Returns:\n",
    "    tuple[list[str], list[float]]: A tuple containing two lists:\n",
    "                                   1. The top 'n' strings most related to the query.\n",
    "                                   2. Their corresponding relatedness scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the embedding for the query string using a pre-defined model.\n",
    "    query_embedding_response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=query,\n",
    "        dimensions=1024,\n",
    "    )\n",
    "    # Extract the embedding data from the response.\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "\n",
    "    # Compute the relatedness of each string in the DataFrame to the query string.\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"combined\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Sort the list of tuples by the relatedness score in descending order (most related first).\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Initialize a list to store the filtered results and a variable to track the last accepted score.\n",
    "    filtered = []\n",
    "    last_score = -1\n",
    "\n",
    "    # Filter strings to meet the threshold criteria and limit the number of results to 'top_n'.\n",
    "    for item in strings_and_relatednesses:\n",
    "        if abs(item[1] - last_score) > threshold:\n",
    "            filtered.append(item)\n",
    "            last_score = item[1]\n",
    "        if len(filtered) >= top_n:\n",
    "            break\n",
    "\n",
    "    # Unzip the tuples to separate strings and their relatedness scores.\n",
    "    strings, relatednesses = zip(*filtered)\n",
    "\n",
    "    # Return only the top 'n' results as specified by the function arguments.\n",
    "    return strings[:top_n], relatednesses[:top_n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code performs a search to find the top 5 items related to the phrase \"dogs dislike product\" \n",
    "# from a DataFrame 'df_search' using the previously defined 'strings_ranked_by_relatedness' function.\n",
    "\n",
    "# Call the function 'strings_ranked_by_relatedness' with the query string, DataFrame, and specify the number \n",
    "# of top related items to return (top_n=5).\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"dislike the food\", df_search, top_n=5)\n",
    "\n",
    "# Loop over each string and its corresponding relatedness score.\n",
    "# The 'zip' function combines the two lists 'strings' and 'relatednesses' so that items from both lists \n",
    "# can be accessed in a single loop iteration.\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    # Print a formatted string that includes a visual separator and the relatedness score formatted to three decimal places.\n",
    "    print(f\"\\n========================\\n{relatedness=:.3f}\")\n",
    "    \n",
    "    # The 'display' function is typically used in Jupyter Notebooks or similar environments to render objects in a more\n",
    "    # visually appealing manner than the basic print function. Here, it is used to display the string from the DataFrame.\n",
    "    display(string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
