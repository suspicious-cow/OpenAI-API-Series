{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Part 3\n",
    "# Going Through the Use Cases\n",
    "Here we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Related third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from ast import literal_eval\n",
    "\n",
    "# Local application/library specific imports\n",
    "import tiktoken\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI client\n",
    "# This assumes you have set an environment variable called OPENAI_API_KEY\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see what trying to answer without using RAG looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# an example question about the Amazon reviews\n",
    "query = \"What people hated their food?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You answer questions about Amazon fine food reviews.'},\n",
    "        {'role': 'user', 'content': query},\n",
    "    ],\n",
    "    model='gpt-4-turbo',\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Embeddings\n",
    "The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:\n",
    "\n",
    "| PRODUCT ID  | USER ID        | SCORE | SUMMARY             | TEXT                                           |\n",
    "|-------------|----------------|-------|---------------------|------------------------------------------------|\n",
    "| B001E4KFG0  | A3SGXH7AUHU8GW | 5     | Good Quality Dog Food | I have bought several of the Vitality canned... |\n",
    "| B00813GRG4  | A1D87F6ZCVE5NK | 1     | Not as Advertised   | Product arrived labeled as Jumbo Salted Peanut... |\n",
    "\n",
    "\n",
    "\n",
    "We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to get the file size\n",
    "def get_file_size(file_path):\n",
    "    \"\"\" Returns the size of the file in megabytes. \"\"\"\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)  # Convert from bytes to megabytes\n",
    "    return size_mb\n",
    "\n",
    "# main utility function to get the embeddings\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    # Replace newlines in the text with spaces for consistent formatting\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Request the embedding for the cleaned text and return the embedding\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# utility function to get the embeddings with reduced dimensions\n",
    "def get_embedding_reduced_dims(text, model=\"text-embedding-3-large\"):\n",
    "    # Replace newlines in the text with spaces for consistent formatting\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Request the embedding for the cleaned text and return the embedding\n",
    "    return client.embeddings.create(input=[text], model=model,dimensions=1024).data[0].embedding\n",
    "\n",
    "# utility function to get the embeddings with reduced dimensions\n",
    "def get_embedding_large(text, model=\"text-embedding-3-large\"):\n",
    "    # Replace newlines in the text with spaces for consistent formatting\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Request the embedding for the cleaned text and return the embedding\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the input data file\n",
    "input_data_path = \"./EmbeddingsDemoAssets/fine_food_reviews_1k.csv\"\n",
    "\n",
    "# Read the CSV file using pandas and set the first column as the index\n",
    "df = pd.read_csv(input_data_path, index_col=0)\n",
    "\n",
    "# Select only the relevant columns from the dataframe\n",
    "df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Combine the 'Summary' and 'Text' columns into a new column with a formatted string\n",
    "df[\"combined\"] = (\n",
    "    \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n",
    ")\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Model Embedding\n",
    "Now let's embed using the small model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the `get_embedding` function to each entry in the 'combined' column and store the results\n",
    "df['embedding'] = df['combined'].apply(lambda x: get_embedding(x))\n",
    "\n",
    "# Save the dataframe with embeddings to a CSV file, omitting the index\n",
    "df.to_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_1k.csv', index=False)\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the data file\n",
    "file_path = './EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_1k.csv'\n",
    "size_mb = get_file_size(file_path)\n",
    "print(f\"The size of the file is {size_mb:.2f} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Model Embedding\n",
    "Let's embed using the large model next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the `get_embedding_large` function to each entry in the 'combined' column and store the results\n",
    "df['embedding'] = df['combined'].apply(lambda x: get_embedding_large(x))\n",
    "\n",
    "# Save the dataframe with embeddings to a CSV file, omitting the index\n",
    "df.to_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_large_1k.csv', index=False)\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the data file\n",
    "file_path = './EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_large_1k.csv'\n",
    "size_mb = get_file_size(file_path)\n",
    "print(f\"The size of the file is {size_mb:.2f} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "Here we show how to load the data into a dataframe from a CSV file to make it ready to be used again when needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_load = pd.read_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_large_1k.csv')\n",
    "\n",
    "# Convert the string representation of embeddings in the 'embedding' column to numpy arrays\n",
    "# By converting the embeddings from string format to numpy arrays immediately after loading, \n",
    "# we ensure that the data is in a ready-to-use state for any subsequent analysis or processing steps.\n",
    "df_load['embedding'] = df_load['embedding'].apply(eval).apply(np.array)\n",
    "\n",
    "# Display the first 5 rows of the loaded dataframe\n",
    "df_load.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the Dimensions\n",
    "Using larger embeddings, for example storing them in a vector store for retrieval, generally costs more and consumes more compute, memory and storage than using smaller embeddings.\n",
    "\n",
    "Both of our new embedding models were trained with a technique that allows developers to trade-off performance and cost of using embeddings. Specifically, developers can shorten embeddings (i.e. remove some numbers from the end of the sequence) without the embedding losing its concept-representing properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the input data file\n",
    "input_data_path = \"./EmbeddingsDemoAssets/fine_food_reviews_1k.csv\"\n",
    "\n",
    "# Read the CSV file using pandas and set the first column as the index\n",
    "df_reduced_dims = pd.read_csv(input_data_path, index_col=0)\n",
    "\n",
    "# Select only the relevant columns from the dataframe\n",
    "df_reduced_dims = df_reduced_dims[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_reduced_dims = df_reduced_dims.dropna()\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_reduced_dims = df_reduced_dims.drop_duplicates()\n",
    "\n",
    "# Combine the 'Summary' and 'Text' columns into a new column with a formatted string\n",
    "df_reduced_dims[\"combined\"] = (\n",
    "    \"Title: \" + df_reduced_dims.Summary.str.strip() + \"; Content: \" + df_reduced_dims.Text.str.strip()\n",
    ")\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df_reduced_dims.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the `get_embedding` function to each entry in the 'combined' column and store the results\n",
    "df_reduced_dims['embedding'] = df_reduced_dims['combined'].apply(lambda x: get_embedding_reduced_dims(x))\n",
    "\n",
    "# Save the dataframe with embeddings to a CSV file, omitting the index\n",
    "df_reduced_dims.to_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_reduced_dims_1k.csv', index=False)\n",
    "\n",
    "# Display the first 5 rows of the modified dataframe\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the size of the data file\n",
    "file_path = './EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_reduced_dims_1k.csv'\n",
    "size_mb = get_file_size(file_path)\n",
    "print(f\"The size of the file is {size_mb:.2f} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Using Embeddings-Based Search\n",
    "There are many common cases where the model is not trained on data which contains key facts and information you want to make accessible when generating responses to a user query. One way of solving this is to put additional information into the context window of the model. This is effective in many use cases but leads to higher token costs. The other way is to use RAG to obtain the information.\n",
    "\n",
    "First, let's load up the file with the (large) embeddings in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_search = pd.read_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_large_1k.csv')\n",
    "\n",
    "# Convert the string representation of embeddings in the 'embedding' column to numpy arrays\n",
    "# By converting the embeddings from string format to numpy arrays immediately after loading, \n",
    "# we ensure that the data is in a ready-to-use state for any subsequent analysis or processing steps.\n",
    "df_search['embedding'] = df_search['embedding'].apply(eval).apply(np.array)\n",
    "\n",
    "# Display the first 5 rows of the loaded dataframe\n",
    "df_search.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relatedness\n",
    "Now let's write some code to show the related reviews based on a ranking score. In this case, we will set up a function to take in our query string and compute the relateness scores for reviews. Then it will show us the top 5 revies in terms of relatedness score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function that ranks strings from a pandas DataFrame based on their relatedness to a given query string.\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - distance.cosine(x, y), # use cosine similarity as the relatedness function\n",
    "    top_n: int = 100,\n",
    "    threshold: float = 0.001  # Minimum score difference to consider for ranking\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"\n",
    "    Retrieve the top 'n' strings related to a query string from a DataFrame, based on a custom relatedness function.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The string to compare other strings against.\n",
    "    df (pd.DataFrame): DataFrame containing the strings and their embeddings.\n",
    "    relatedness_fn (callable): A function that computes the relatedness score between two embeddings. By default, \n",
    "                               it uses the cosine similarity between embeddings.\n",
    "    top_n (int): The number of top related strings to return.\n",
    "    threshold (float): The minimum difference between scores needed to consider one string more related than another.\n",
    "\n",
    "    Returns:\n",
    "    tuple[list[str], list[float]]: A tuple containing two lists:\n",
    "                                   1. The top 'n' strings most related to the query.\n",
    "                                   2. Their corresponding relatedness scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the embedding for the query string using the large model.\n",
    "    query_embedding_response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=query,\n",
    "    )\n",
    "    # Extract the embedding data from the response.\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "\n",
    "    # Compute the relatedness of each string in the DataFrame to the query string.\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"combined\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Sort the list of tuples by the relatedness score in descending order (most related first).\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Initialize a list to store the filtered results and a variable to track the last accepted score.\n",
    "    filtered = []\n",
    "    last_score = -1\n",
    "\n",
    "    # Filter strings to meet the threshold criteria and limit the number of results to 'top_n'.\n",
    "    for item in strings_and_relatednesses:\n",
    "        if abs(item[1] - last_score) > threshold:\n",
    "            filtered.append(item)\n",
    "            last_score = item[1]\n",
    "        if len(filtered) >= top_n:\n",
    "            break\n",
    "\n",
    "    # Unzip the tuples to separate strings and their relatedness scores.\n",
    "    strings, relatednesses = zip(*filtered)\n",
    "\n",
    "    # Return only the top 'n' results as specified by the function arguments.\n",
    "    return strings[:top_n], relatednesses[:top_n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code performs a search to find the top 5 items related to the phrase \"don't like the taste\" \n",
    "# from a DataFrame 'df_search' using the previously defined 'strings_ranked_by_relatedness' function.\n",
    "\n",
    "# Call the function 'strings_ranked_by_relatedness' with the query string, DataFrame, and specify the number \n",
    "# of top related items to return (top_n=5).\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"don't like the taste\", df_search, top_n=5)\n",
    "\n",
    "# Loop over each string and its corresponding relatedness score.\n",
    "# The 'zip' function combines the two lists 'strings' and 'relatednesses' so that items from both lists \n",
    "# can be accessed in a single loop iteration.\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    # Print a formatted string that includes a visual separator and the relatedness score formatted to three decimal places.\n",
    "    print(f\"\\n========================\\n{relatedness=:.3f}\")\n",
    "    \n",
    "    # The 'display' function is typically used in Jupyter Notebooks or similar environments to render objects in a more\n",
    "    # visually appealing manner than the basic print function. Here, it is used to display the string from the DataFrame.\n",
    "    display(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking Questions\n",
    "Now that we have the ability to find reviews that are relevant we can ask questions based on the reviews retrieved and put into the context window. We will need a number of items to make sure we are getting good data and that we don't go over our token limit of 128k.\n",
    "\n",
    "Let's start with a function to get a token count for any string passed into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_tokens(text: str, model: str = 'gpt-4-turbo') -> int:\n",
    "    \"\"\"\n",
    "    Return the number of tokens in a string using a specified model's tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to tokenize.\n",
    "        model (str): The model whose tokenizer is used, defaulting to 'gpt-4-turbo'.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens that the text is divided into by the tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the tokenizer configuration specific to the model\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    # Encode the text using the tokenizer and count the number of tokens\n",
    "    return len(encoding.encode(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a function to pull reviews from our dataframe that can be put into the GPT context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(query: str, df: pd.DataFrame, model: str, token_budget: int) -> str:\n",
    "    \"\"\"\n",
    "    Construct a message for GPT using related texts from a DataFrame, staying within a token limit.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The user's query to find related text.\n",
    "        df (pd.DataFrame): DataFrame containing potential related texts.\n",
    "        model (str): Model identifier for tokenization.\n",
    "        token_budget (int): Maximum number of tokens allowed in the final message.\n",
    "\n",
    "    Returns:\n",
    "        str: A message constructed with introductory text, relevant reviews, and the query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve ranked strings and their relatedness from the dataframe\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "\n",
    "    # Introduction to include in the message\n",
    "    introduction = 'Use the below Amazon food reviews to answer the subsequent question. If the answer cannot be found in the reviews, write \"I could not find an answer.\"'\n",
    "\n",
    "    # Formatting the query into a question format\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "\n",
    "    # Initialize the message with the introduction text\n",
    "    message = introduction\n",
    "\n",
    "    # Iterate over each related string to build the message\n",
    "    for string in strings:\n",
    "        # Format the string as an Amazon Food Review\n",
    "        next_article = f'\\n\\nAmazon Food Review:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "\n",
    "        # Check if adding the next article exceeds the token budget\n",
    "        if num_tokens(message + next_article + question, model=model) > token_budget:\n",
    "            break  # Stop adding articles if token budget is exceeded\n",
    "        else:\n",
    "            message += next_article  # Append the article if within budget\n",
    "\n",
    "    # Return the final message with the question appended\n",
    "    return message + question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a function to actually allow us to ask questions from the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str = 'gpt-4-turbo',\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer to a query using GPT, based on a DataFrame of related texts.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The query to be answered.\n",
    "        df (pd.DataFrame): DataFrame containing texts and embeddings.\n",
    "        model (str): Model identifier for tokenization, default is 'gpt-4-turbo'.\n",
    "        token_budget (int): Maximum number of tokens for the generated message, default is 3596.\n",
    "        print_message (bool): Whether to print the constructed message, default is False.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from GPT based on the query and provided texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a message using related texts from the DataFrame\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "\n",
    "    # Optionally print the constructed message\n",
    "    if print_message:\n",
    "        print(message)\n",
    "\n",
    "    # Set up the conversation format for the GPT model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about Amazon Food Reviews.\"},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "\n",
    "    # Request a completion from the GPT model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0  # Use deterministic mode for reproducibility\n",
    "    )\n",
    "\n",
    "    # Extract the response message from the completion\n",
    "    response_message = response.choices[0].message.content\n",
    "\n",
    "    return response_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's ask our question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit our question to the model\n",
    "ask(\"What are the top 3 reviews that indicate people don't like the food? Give me the title and text of the reviews in this format: Title: <title>; Content: <content>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks funky so let's pretty it up a bit. We can always try a variety of techniques, like outputting JSON, but, for now, we will just use regular expressions to deal with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Format a string containing multiple entries with titles and contents into a readable format.\n",
    "\n",
    "    Parameters:\n",
    "        response (str): The string response containing multiple formatted entries.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string where each entry is separated and clearly labeled.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compile a pattern to match entries formatted as \"Title: <title>; Content: <content>\"\n",
    "    pattern = re.compile(r\"Title: (.*?); Content: (.*?)(?=\\nTitle: |$)\", re.S)\n",
    "    \n",
    "    # Extract all matches using the pattern\n",
    "    matches = pattern.findall(response)\n",
    "    \n",
    "    # List to store formatted entries\n",
    "    formatted_lines = []\n",
    "    \n",
    "    # Format each match with proper numbering and formatting\n",
    "    for index, (title, content) in enumerate(matches, start=1):\n",
    "        # Strip and replace newline characters in content\n",
    "        content = content.strip().replace('\\n', ' ')\n",
    "        # Append formatted title and content to the list\n",
    "        formatted_lines.append(f\"{index}. Title: **{title.strip()}**\")\n",
    "        formatted_lines.append(f\"   Content: {content}\")\n",
    "        formatted_lines.append('')  # Blank line for separation\n",
    "\n",
    "    # Join all formatted lines into a single string\n",
    "    return '\\n'.join(formatted_lines)\n",
    "\n",
    "# Example usage with a provided function response\n",
    "response = \"Title: Not tasty; Content: The food was bland and unenjoyable.\\nTitle: Bad Service; Content: Waited 30 minutes to get served.\"\n",
    "formatted_output = format_response(response)\n",
    "print(formatted_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search\n",
    "Now we will do straight searching for similarities using an algorithm called \"cosine similarity\" to determine how similar the words we are looking for are to words in the reviews.\n",
    "\n",
    "First let's load data into our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into our dataframe and turn the embeddings into numpy arrays\n",
    "df_semantic_search = pd.read_csv('./EmbeddingsDemoAssets/fine_food_reviews_with_embeddings_large_1k.csv')\n",
    "\n",
    "df_semantic_search[\"embedding\"] = df_semantic_search.embedding.apply(literal_eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "Now let's use cosine similarity, like we did before, to get relatedness scores for reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code performs a search to find the top 5 items related to the phrase \"don't like the taste\" \n",
    "# from a DataFrame 'df_search' using the previously defined 'strings_ranked_by_relatedness' function.\n",
    "\n",
    "# Call the function 'strings_ranked_by_relatedness' with the query string, DataFrame, and specify the number \n",
    "# of top related items to return (top_n=5).\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"don't like the taste\", df_semantic_search, top_n=5)\n",
    "\n",
    "# Loop over each string and its corresponding relatedness score.\n",
    "# The 'zip' function combines the two lists 'strings' and 'relatednesses' so that items from both lists \n",
    "# can be accessed in a single loop iteration.\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    # Print a formatted string that includes a visual separator and the relatedness score formatted to three decimal places.\n",
    "    print(f\"\\n========================\\n{relatedness=:.3f}\")\n",
    "    \n",
    "    # The 'display' function is typically used in Jupyter Notebooks or similar environments to render objects in a more\n",
    "    # visually appealing manner than the basic print function. Here, it is used to display the string from the DataFrame.\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eucleidean Distance\n",
    "Now let's try Eucleidian Distance to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_strings_by_euclidean(query: str, df: pd.DataFrame, top_n: int = 100) -> tuple[list[str], list[float]]:\n",
    "    \"\"\"\n",
    "    Returns a list of strings and their relatedness scores sorted from most related to least.\n",
    "    The relatedness is determined using the Euclidean distance.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The query string to compare against dataframe strings.\n",
    "        df (pd.DataFrame): DataFrame containing strings and their embeddings.\n",
    "        top_n (int): The number of top related strings to return.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists; one for strings and one for their corresponding relatedness scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the embedding for the query string from a model\n",
    "    query_embedding_response = client.embeddings.create(\n",
    "        model='text-embedding-3-large',\n",
    "        input=query,\n",
    "    )\n",
    "    # Extract the embedding from the response\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "\n",
    "    # Function to calculate negative Euclidean distance between two embeddings\n",
    "    def calculate_relatedness(x, y):\n",
    "        return -distance.euclidean(x, y)\n",
    "\n",
    "    # Create a list of tuples containing strings from the DataFrame and their relatedness to the query\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"combined\"], calculate_relatedness(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()  # Iterate over DataFrame rows\n",
    "    ]\n",
    "\n",
    "    # Sort the list of tuples by relatedness, descending\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Unzip the sorted list into two lists: one for strings and another for relatedness scores\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "\n",
    "    # Return the top N related strings and their scores\n",
    "    return strings[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code performs a search to find the top 5 items related to the phrase \"don't like the taste\" \n",
    "# from a DataFrame 'df_search' using the previously defined 'strings_ranked_by_relatedness' function.\n",
    "\n",
    "# Call the function 'strings_ranked_by_relatedness_ed' with the query string, DataFrame, and specify the number \n",
    "# of top related items to return (top_n=5).\n",
    "strings, relatednesses = strings_ranked_by_relatedness_ed(\"don't like the taste\", df_semantic_search, top_n=5)\n",
    "\n",
    "# Loop over each string and its corresponding relatedness score.\n",
    "# The 'zip' function combines the two lists 'strings' and 'relatednesses' so that items from both lists \n",
    "# can be accessed in a single loop iteration.\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    # Print a formatted string that includes a visual separator and the relatedness score formatted to three decimal places.\n",
    "    print(f\"\\n========================\\n{relatedness=:.3f}\")\n",
    "    \n",
    "    # The 'display' function is typically used in Jupyter Notebooks or similar environments to render objects in a more\n",
    "    # visually appealing manner than the basic print function. Here, it is used to display the string from the DataFrame.\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Example\n",
    "Because shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation.\n",
    "\n",
    "Below, we illustrate a basic recommender. It takes in a list of strings and one 'source' string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, we apply a version of this function to the AG news dataset (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article.\n",
    "\n",
    "### Load the Data\n",
    "First, let's load our data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from dataset\n",
    "# full dataset available at http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html\n",
    "dataset_path = \"./EmbeddingsDemoAssets/AG_news_samples.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Number of examples to display\n",
    "n_examples = 5\n",
    "\n",
    "# Print the title, description, and label of the top n examples\n",
    "for idx, row in df.head(n_examples).iterrows():\n",
    "    print(\"\\nTitle:\", row['title'])\n",
    "    print(\"Description:\", row['description'])\n",
    "    print(\"Label:\", row['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Cache to Save Embeddings\n",
    "Before getting embeddings for these articles, let's set up a cache to save the embeddings we generate. In general, it's a good idea to save your embeddings so you can re-use them later. If you don't save them, you'll pay again each time you compute them again.\n",
    "\n",
    "The cache is a dictionary that maps tuples of (text, model) to an embedding, which is a list of floats. The cache is saved as a Python pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the embedding cache file\n",
    "embedding_cache_path = \"./EmbeddingsDemoAssets/recommendations_embeddings_cache.pkl\"\n",
    "\n",
    "# Try to load the embedding cache if it exists, otherwise initialize an empty dictionary\n",
    "try:\n",
    "    with open(embedding_cache_path, \"rb\") as embedding_cache_file:\n",
    "        embedding_cache = pickle.load(embedding_cache_file)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "\n",
    "# Define a function to retrieve embeddings from the cache or request via the API if not found\n",
    "def embedding_from_string(string: str, model: str = \"text-embedding-3-large\", cache=embedding_cache) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve the embedding of a given string using a specified model.\n",
    "    Utilizes a cache to avoid redundant API calls.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The string to embed.\n",
    "        model (str): The model to use for embedding.\n",
    "        cache (dict): The cache to store and retrieve embeddings.\n",
    "\n",
    "    Returns:\n",
    "        list: The embedding vector of the string.\n",
    "    \"\"\"\n",
    "    # Check if the embedding is already in the cache\n",
    "    if (string, model) not in cache:\n",
    "        # Assume get_embedding_large is a function that requests the embedding from an API\n",
    "        cache[(string, model)] = get_embedding_large(string, model)\n",
    "        # Save the updated cache to disk\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(cache, embedding_cache_file)\n",
    "    return cache[(string, model)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure everything is working by getting an embeddding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Retrieving and displaying embeddings for descriptions from the dataset\n",
    "\n",
    "# Fetch the first description from the dataset and print it\n",
    "example_string = df[\"description\"].iloc[0]  # Use iloc for safer access by index\n",
    "print(f\"\\nExample string: {example_string}\")\n",
    "\n",
    "# Retrieve the embedding for the first description and print the first 10 dimensions\n",
    "example_embedding = embedding_from_string(example_string)\n",
    "print(f\"\\nExample embedding: {example_embedding[:10]}...\")\n",
    "\n",
    "# Fetch the tenth description from the dataset and print it\n",
    "example_string_10 = df[\"description\"].iloc[9]  # Use iloc for safer access by index\n",
    "print(f\"\\nExample string: {example_string_10}\")\n",
    "\n",
    "# Retrieve the embedding for the tenth description and print the first 10 dimensions\n",
    "example_embedding_10 = embedding_from_string(example_string_10)\n",
    "print(f\"\\nExample embedding: {example_embedding_10[:10]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend Similar Articles Based on Embeddings\n",
    "To find similar articles, let's follow a three-step plan:\n",
    "\n",
    "1. Get the similarity embeddings of all the article descriptions\n",
    "2. Calculate the distance between a source title and all other articles\n",
    "3. Print out the other articles closest to the source title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Calculate distances from the query embedding to each embedding in the list.\n",
    "\n",
    "    Parameters:\n",
    "        query_embedding (array): The embedding of the query string.\n",
    "        embeddings (list of arrays): A list of embeddings to compare against.\n",
    "        distance_metric (str): The metric used to calculate distance, default is \"cosine\".\n",
    "\n",
    "    Returns:\n",
    "        list: Distances from the query embedding to each embedding in the list.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for emb in embeddings:\n",
    "        dist = distance.cosine(query_embedding, emb) if emb is not None else np.inf\n",
    "        distances.append(dist)\n",
    "    return distances\n",
    "\n",
    "def indices_of_nearest_neighbors_from_distances(distances, k=1):\n",
    "    \"\"\"\n",
    "    Return indices of the k nearest neighbors based on the given distances.\n",
    "\n",
    "    Parameters:\n",
    "        distances (list): List of distances from the query.\n",
    "        k (int): Number of nearest neighbors to return.\n",
    "\n",
    "    Returns:\n",
    "        array: Indices of the k nearest neighbors.\n",
    "    \"\"\"\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    return sorted_indices[1:k+1]  # Skip the first index assuming it's the query itself\n",
    "\n",
    "def print_recommendations_from_strings(strings, index_of_source_string, model='text-embedding-3-large', k_nearest_neighbors=1):\n",
    "    \"\"\"\n",
    "    Print out the k nearest neighbors of a given string based on embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        strings (list): List of strings to consider for neighbors.\n",
    "        index_of_source_string (int): Index of the string to find neighbors for.\n",
    "        model (str): Model used for generating embeddings.\n",
    "        k_nearest_neighbors (int): Number of nearest neighbors to print.\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of the nearest neighbors.\n",
    "    \"\"\"\n",
    "    embeddings = [embedding_from_string(string, model=model) for string in strings]\n",
    "    query_embedding = embeddings[index_of_source_string]\n",
    "    distances = distances_from_embeddings(query_embedding, embeddings)\n",
    "    indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances, k_nearest_neighbors + 1)\n",
    "\n",
    "    query_string = strings[index_of_source_string]\n",
    "    print(f\"Source string: {query_string}\")\n",
    "    \n",
    "    # Filter out the source string index and print recommendations\n",
    "    recommendations = [i for i in indices_of_nearest_neighbors if i != index_of_source_string]\n",
    "    for idx, rec_idx in enumerate(recommendations[:k_nearest_neighbors], 1):\n",
    "        print(f\"\"\"\n",
    "        --- Recommendation #{idx} (Nearest Neighbor #{idx} of {k_nearest_neighbors}) ---\n",
    "        String: {strings[rec_idx]}\n",
    "        Distance: {distances[rec_idx]:0.3f}\n",
    "        \"\"\")\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This code block may take a few minutes to run\n",
    "article_descriptions = df[\"description\"].tolist()\n",
    "\n",
    "tony_blair_articles = print_recommendations_from_strings(\n",
    "    strings=article_descriptions,  # let's base similarity off of the article description\n",
    "    index_of_source_string=0,  # articles similar to the first one about Tony Blair\n",
    "    k_nearest_neighbors=5,  # 5 most similar articles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This code block may take a few minutes to run\n",
    "chipset_security_articles = print_recommendations_from_strings(\n",
    "    strings=article_descriptions,  # let's base similarity off of the article description\n",
    "    index_of_source_string=1,  # let's look at articles similar to the second one about a more secure chipset\n",
    "    k_nearest_neighbors=5,  # let's look at the 5 most similar articles\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put The Results into JSON Format\n",
    "JSON is a popular structured format that is used worldwide for transmitting text information. Let's put our Tony Blair articles in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of dictionaries for each selected article, including available details\n",
    "articles_json = [\n",
    "    {\n",
    "        'title': df.loc[index, 'title'],\n",
    "        'description': df.loc[index, 'description']\n",
    "    } for index in tony_blair_articles\n",
    "]\n",
    "\n",
    "# Optionally convert to a JSON string if needed for file storage or transmission\n",
    "articles_json_string = json.dumps(articles_json, indent=4)\n",
    "\n",
    "print(articles_json_string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
