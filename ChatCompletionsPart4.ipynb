{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Completions Part 4\n",
    "\n",
    "Make sure you have an environment variable called OPENAI_API_KEY set with your API key.\n",
    "\n",
    "## logprobs (boolean or null)\n",
    "Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.\n",
    "\n",
    "### Why Should you care?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansive mystery.\n"
     ]
    }
   ],
   "source": [
    "# normal chat example without logprob data\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Use two words to describe the universe.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    stop=None,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansive mystery.\n",
      "\n",
      "\n",
      "\n",
      "ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Exp', bytes=[69, 120, 112], logprob=-2.120557, top_logprobs=[]), ChatCompletionTokenLogprob(token='ans', bytes=[97, 110, 115], logprob=-0.000444374, top_logprobs=[]), ChatCompletionTokenLogprob(token='ive', bytes=[105, 118, 101], logprob=-1.18755715e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' mystery', bytes=[32, 109, 121, 115, 116, 101, 114, 121], logprob=-0.2989575, top_logprobs=[]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.15134661, top_logprobs=[])])\n",
      "\n",
      "\n",
      "\n",
      "Exp\n",
      "-2.120557\n",
      "12.00%\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chat example with extracting one set of logprob data\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Use two words to describe the universe.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    stop=None,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    logprobs=True,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "print(\"\\n\\n\")\n",
    "print(completion.choices[0].logprobs)\n",
    "print(\"\\n\\n\")\n",
    "print(completion.choices[0].logprobs.content[0].token)\n",
    "print(completion.choices[0].logprobs.content[0].logprob)\n",
    "\n",
    "# calculating the probability of the token\n",
    "probability = np.round(np.exp(completion.choices[0].logprobs.content[0].logprob)*100,2)\n",
    "message = f\"{probability}%\" if probability < 0.01 else f\"{probability:.2f}%\"\n",
    "print(message)\n",
    "print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Infinitely vast.\n",
      "\n",
      "Token probabilities:\n",
      "\n",
      "Token: Inf, Logprob: -2.0300112, Probability: 13.13%\n",
      "Token: initely, Logprob: -1.3856493e-06, Probability: 100.00%\n",
      "Token:  vast, Logprob: -0.7190319, Probability: 48.72%\n",
      "Token: ., Logprob: -0.9741263, Probability: 37.75%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI()\n",
    "\n",
    "# Create a chat completion request\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[ \n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Use two words to describe the universe.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    stop=None,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    logprobs=True,\n",
    ")\n",
    "\n",
    "# Retrieve the response\n",
    "response = completion.choices[0].message.content\n",
    "logprobs = completion.choices[0].logprobs\n",
    "\n",
    "print(\"Response:\", response)\n",
    "print(\"\\nToken probabilities:\\n\")\n",
    "\n",
    "# Iterate through each token logprob entry\n",
    "for current_logprob in logprobs.content:\n",
    "    token = current_logprob.token\n",
    "    logprob = current_logprob.logprob\n",
    "    probability = np.round(np.exp(logprob) * 100, 2)\n",
    "    message = f\"{probability}%\" if probability < 0.01 else f\"{probability:.2f}%\"\n",
    "    print(f\"Token: {token}, Logprob: {logprob}, Probability: {message}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top_logprobs \n",
    "An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vast, mysterious\n",
      "\n",
      "\n",
      "\n",
      "ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='V', bytes=[86], logprob=-0.45684347, top_logprobs=[TopLogprob(token='V', bytes=[86], logprob=-0.45684347), TopLogprob(token='Inf', bytes=[73, 110, 102], logprob=-1.6443435)]), ChatCompletionTokenLogprob(token='ast', bytes=[97, 115, 116], logprob=0.0, top_logprobs=[TopLogprob(token='ast', bytes=[97, 115, 116], logprob=0.0), TopLogprob(token='as', bytes=[97, 115], logprob=-18.3125)]), ChatCompletionTokenLogprob(token=',', bytes=[44], logprob=-1.3542051, top_logprobs=[TopLogprob(token=' mystery', bytes=[32, 109, 121, 115, 116, 101, 114, 121], logprob=-0.41670513), TopLogprob(token=',', bytes=[44], logprob=-1.3542051)]), ChatCompletionTokenLogprob(token=' mysterious', bytes=[32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115], logprob=-0.014296548, top_logprobs=[TopLogprob(token=' mysterious', bytes=[32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115], logprob=-0.014296548), TopLogprob(token=' M', bytes=[32, 77], logprob=-4.4830465)])])\n",
      "\n",
      "\n",
      "\n",
      "V\n",
      "-0.45684347\n",
      "63.33%\n",
      "\n",
      "\n",
      "Inf\n",
      "-1.6443435\n",
      "19.31%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chat example with extracting one set of logprob data\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Use two words to describe the universe.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    stop=None,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    logprobs=True,\n",
    "    top_logprobs=2,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "print(\"\\n\\n\")\n",
    "print(completion.choices[0].logprobs)\n",
    "print(\"\\n\\n\")\n",
    "print(completion.choices[0].logprobs.content[0].top_logprobs[0].token)\n",
    "print(completion.choices[0].logprobs.content[0].top_logprobs[0].logprob)\n",
    "# calculating the probability of the token\n",
    "probability = np.round(np.exp(completion.choices[0].logprobs.content[0].top_logprobs[0].logprob)*100,2)\n",
    "message = f\"{probability}%\" if probability < 0.01 else f\"{probability:.2f}%\"\n",
    "print(message)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(completion.choices[0].logprobs.content[0].top_logprobs[1].token)\n",
    "print(completion.choices[0].logprobs.content[0].top_logprobs[1].logprob)\n",
    "# calculating the probability of the token\n",
    "probability = np.round(np.exp(completion.choices[0].logprobs.content[0].top_logprobs[1].logprob)*100,2)\n",
    "message = f\"{probability}%\" if probability < 0.01 else f\"{probability:.2f}%\"\n",
    "print(message)\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vast mystery.\n",
      "\n",
      "\n",
      "\n",
      "Token: V\n",
      "Top Logprobs:\n",
      "  V: -0.45684347\n",
      "  Probability: 63.33%\n",
      "  Inf: -1.6443435\n",
      "  Probability: 19.31%\n",
      "  Exp: -2.3005934\n",
      "  Probability: 10.02%\n",
      "  In: -2.7068434\n",
      "  Probability: 6.67%\n",
      "  End: -6.1443434\n",
      "  Probability: 0.21%\n",
      "\n",
      "\n",
      "Token: ast\n",
      "Top Logprobs:\n",
      "  ast: 0.0\n",
      "  Probability: 100.0%\n",
      "  as: -18.328125\n",
      "  Probability: 0.0%\n",
      "  AST: -18.765625\n",
      "  Probability: 0.0%\n",
      "   ast: -21.3125\n",
      "  Probability: 0.0%\n",
      "  aste: -22.609375\n",
      "  Probability: 0.0%\n",
      "\n",
      "\n",
      "Token:  mystery\n",
      "Top Logprobs:\n",
      "   mystery: -0.4577507\n",
      "  Probability: 63.27%\n",
      "  ,: -1.2702507\n",
      "  Probability: 28.08%\n",
      "   Mystery: -3.1765008\n",
      "  Probability: 4.17%\n",
      "  ly: -4.020251\n",
      "  Probability: 1.79%\n",
      "  .: -4.364001\n",
      "  Probability: 1.27%\n",
      "\n",
      "\n",
      "Token: .\n",
      "Top Logprobs:\n",
      "  <|end|>: -0.49822634\n",
      "  Probability: 60.76%\n",
      "  .: -0.93572634\n",
      "  Probability: 39.23%\n",
      "  <|end|>: -9.716976\n",
      "  Probability: 0.01%\n",
      "   : -10.451351\n",
      "  Probability: 0.0%\n",
      "  .\n",
      ": -13.420101\n",
      "  Probability: 0.0%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Create a chat completion request\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[ \n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Use two words to describe the universe.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    stop=None,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,  # Change this to fetch all top logprobs\n",
    ")\n",
    "\n",
    "# Print the resulting message\n",
    "print(completion.choices[0].message.content)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Loop through each token logprob and print all the top logprobs for each token\n",
    "for token_logprob in completion.choices[0].logprobs.content:\n",
    "    print(f\"Token: {token_logprob.token}\")\n",
    "    print(\"Top Logprobs:\")\n",
    "    for top_logprob in token_logprob.top_logprobs:\n",
    "        print(f\"  {top_logprob.token}: {top_logprob.logprob}\")\n",
    "        probability = np.round(np.exp(top_logprob.logprob)*100,2)\n",
    "        print(f\"  Probability: {probability}%\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
